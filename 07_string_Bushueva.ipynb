{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в обработку текста на естественном языке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расстояние редактирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Загрузите предобработанные описания рецептов из файла `preprocessed_descriptions.csv`. Получите набор уникальных слов `words`, содержащихся в текстах описаний рецептов (воспользуйтесь `word_tokenize` из `nltk`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['string',\n",
       " 'can',\n",
       " 'the',\n",
       " 'substrings',\n",
       " 'tokenizers',\n",
       " 'divide',\n",
       " 'in',\n",
       " 'example',\n",
       " 'and',\n",
       " 'Tokenizers',\n",
       " 'into',\n",
       " 'a',\n",
       " 'punctuation',\n",
       " 'be',\n",
       " 'strings',\n",
       " 'lists',\n",
       " 'words',\n",
       " '.',\n",
       " 'to',\n",
       " 'For',\n",
       " 'find',\n",
       " ',',\n",
       " 'of',\n",
       " 'used']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "text = 'Tokenizers divide strings into lists of substrings. For example, tokenizers can be used to find the words and punctuation in a string.'\n",
    "words = list(set(nltk.word_tokenize(text)))\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Сгенерируйте 5 пар случайно выбранных слов и посчитайте между ними расстояние редактирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пара: ['слово6', 'слово4'], расстояние:1\n",
      "Пара: ['слово7', 'слово3'], расстояние:1\n",
      "Пара: ['слово7', 'слово4'], расстояние:1\n",
      "Пара: ['слово4', 'слово7'], расстояние:1\n",
      "Пара: ['слово3', 'слово5'], расстояние:1\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "from nltk.metrics import *\n",
    "our_lst = ['слово1', 'слово2', 'слово3', 'слово4', 'слово5', 'слово6', 'слово7']\n",
    "lst = [0]*5\n",
    "for i in range(5):\n",
    "    lst[i] = random.sample(our_lst,2)\n",
    "    print(f'Пара: {lst[i]}, расстояние:{edit_distance(lst[i][0], lst[i][1])}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 Напишите функцию, которая для заданного слова `word` возвращает `k` ближайших к нему слов из списка `words` (близость слов измеряется с помощью расстояния Левенштейна)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['слово3', 'слово1']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_nearest_word(word, words, k):\n",
    "    dct = {w: edit_distance(w, word) for w in words}\n",
    "    lst = sorted(dct, key=dct.get)[:k]\n",
    "    return lst\n",
    "\n",
    "find_nearest_word('слово3', our_lst, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стемминг, лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 На основе результатов 1.1 создайте `pd.DataFrame` со столбцами: \n",
    "    * word\n",
    "    * stemmed_word \n",
    "    * normalized_word \n",
    "\n",
    "Столбец `word` укажите в качестве индекса. \n",
    "\n",
    "Для стемминга воспользуйтесь `SnowballStemmer`, для нормализации слов - `WordNetLemmatizer`. Сравните результаты стемминга и лемматизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed_word</th>\n",
       "      <th>normalized_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>string</th>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>can</th>\n",
       "      <td>can</td>\n",
       "      <td>can</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>substrings</th>\n",
       "      <td>substr</td>\n",
       "      <td>substring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokenizers</th>\n",
       "      <td>token</td>\n",
       "      <td>tokenizers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>divide</th>\n",
       "      <td>divid</td>\n",
       "      <td>divide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>example</th>\n",
       "      <td>exampl</td>\n",
       "      <td>example</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tokenizers</th>\n",
       "      <td>token</td>\n",
       "      <td>Tokenizers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>into</th>\n",
       "      <td>into</td>\n",
       "      <td>into</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>punctuation</th>\n",
       "      <td>punctuat</td>\n",
       "      <td>punctuation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>be</th>\n",
       "      <td>be</td>\n",
       "      <td>be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strings</th>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lists</th>\n",
       "      <td>list</td>\n",
       "      <td>list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>words</th>\n",
       "      <td>word</td>\n",
       "      <td>word</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>For</th>\n",
       "      <td>for</td>\n",
       "      <td>For</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>find</th>\n",
       "      <td>find</td>\n",
       "      <td>find</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>used</th>\n",
       "      <td>use</td>\n",
       "      <td>used</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            stemmed_word normalized_word\n",
       "string            string          string\n",
       "can                  can             can\n",
       "the                  the             the\n",
       "substrings        substr       substring\n",
       "tokenizers         token      tokenizers\n",
       "divide             divid          divide\n",
       "in                    in              in\n",
       "example           exampl         example\n",
       "and                  and             and\n",
       "Tokenizers         token      Tokenizers\n",
       "into                into            into\n",
       "a                      a               a\n",
       "punctuation     punctuat     punctuation\n",
       "be                    be              be\n",
       "strings           string          string\n",
       "lists               list            list\n",
       "words               word            word\n",
       ".                      .               .\n",
       "to                    to              to\n",
       "For                  for             For\n",
       "find                find            find\n",
       ",                      ,               ,\n",
       "of                    of              of\n",
       "used                 use            used"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import SnowballStemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "df = pd.DataFrame({'stemmed_word': [stemmer.stem(word) for word in words], 'normalized_word': [lemmatizer.lemmatize(word) for word in words]}, index=words)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Удалите стоп-слова из описаний рецептов. Какую долю об общего количества слов составляли стоп-слова? Сравните топ-10 самых часто употребляемых слов до и после удаления стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizers',\n",
       " 'divide',\n",
       " 'strings',\n",
       " 'lists',\n",
       " 'substrings',\n",
       " '.',\n",
       " 'example',\n",
       " ',',\n",
       " 'tokenizers',\n",
       " 'used',\n",
       " 'find',\n",
       " 'words',\n",
       " 'punctuation',\n",
       " 'string',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "text = 'Tokenizers divide strings into lists of substrings. For example, tokenizers can be used to find the words and punctuation in a string.'\n",
    "tokens = word_tokenize(text.lower())\n",
    "english_stopwords = stopwords.words('english')\n",
    "tokens_without_stopwords = [word for word in tokens if word not in english_stopwords]\n",
    "tokens_without_stopwords"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
